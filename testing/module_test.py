#!/usr/bin/env python3
"""
Comprehensive Module Test Suite for statz

This test suite covers all module functionality including:
- System specifications retrieval (get_system_specs)
- Real-time hardware usage monitoring (get_hardware_usage)
- Process monitoring and analysis (get_top_n_processes)
- Temperature sensor readings (get_system_temps)~
- System health scoring (system_health_score)
- Performance benchmarking (cpu_benchmark, mem_benchmark, disk_benchmark)
- Export functionality (JSON and CSV)
- Error handling and edge cases
- Cross-platform compatibility
- Performance and stress testing

Usage:
    python module_test.py                    # Run all tests
    python module_test.py --specs            # Run system specs tests only
    python module_test.py --usage            # Run hardware usage tests only
    python module_test.py --processes        # Run process monitoring tests only
    python module_test.py --temp             # Run temperature tests only
    python module_test.py --health           # Run health scoring tests only
    python module_test.py --benchmark        # Run benchmarking tests only
    python module_test.py --export           # Run export functionality tests only
    python module_test.py --stress           # Run stress/performance tests only
    python module_test.py --verbose          # Verbose output
"""

import sys
import os
import time
import json
import csv
import platform
import argparse
import tempfile
import glob
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from unittest.mock import patch, MagicMock

# Add the parent directory to the Python path to import statz
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from statz.stats import get_hardware_usage, get_system_specs, get_top_n_processes, __version__
    from statz.benchmark import cpu_benchmark, mem_benchmark, disk_benchmark
    from statz.temp import get_system_temps
    from statz.health import system_health_score
    from statz.file import export_into_file
except ImportError as e:
    print(f"âŒ Failed to import statz module: {e}")
    sys.exit(1)

class Colors:
    """ANSI color codes for terminal output"""
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

class TestResult:
    """Container for test results"""
    def __init__(self, name: str, passed: bool, output: str = "", error: str = "", duration: float = 0.0):
        self.name = name
        self.passed = passed
        self.output = output
        self.error = error
        self.duration = duration

class ModuleTester:
    """Comprehensive module testing class"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.results: List[TestResult] = []
        self.current_os = platform.system()
        
        # Test output directory
        self.test_output_dir = Path("test_outputs")
        self.test_output_dir.mkdir(exist_ok=True)
        
        print(f"{Colors.CYAN}ğŸ§ª statz Module Test Suite{Colors.END}")
        print(f"{Colors.BLUE}Platform: {self.current_os}{Colors.END}")
        print(f"{Colors.BLUE}statz Version: {__version__}{Colors.END}")
        print("-" * 60)
    
    def run_test(self, test_func, test_name: str, *args, **kwargs) -> TestResult:
        """Run a single test function and return result"""
        start_time = time.time()
        
        try:
            result = test_func(*args, **kwargs)
            duration = time.time() - start_time
            
            # Validate result based on test
            if result is None:
                # Some functions like get_system_temps can return None
                passed = True
                output = "Function returned None (acceptable for some functions)"
            elif isinstance(result, (dict, list, tuple, str, int, float)):
                passed = True
                output = f"Function returned {type(result).__name__}: {str(result)[:100]}{'...' if len(str(result)) > 100 else ''}"
            else:
                passed = False
                output = f"Unexpected return type: {type(result)}"
            
            return TestResult(test_name, passed, output, "", duration)
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(test_name, False, "", str(e), duration)
    
    def add_result(self, result: TestResult):
        """Add a test result"""
        self.results.append(result)
        
        # Print immediate result
        status = f"{Colors.GREEN}âœ… PASS{Colors.END}" if result.passed else f"{Colors.RED}âŒ FAIL{Colors.END}"
        duration_str = f"({result.duration:.2f}s)"
        print(f"{status} {result.name} {duration_str}")
        
        if not result.passed and (result.error or (self.verbose and result.output)):
            if result.error:
                print(f"    {Colors.RED}Error: {result.error}{Colors.END}")
            if self.verbose and result.output:
                print(f"    {Colors.YELLOW}Output: {result.output}{Colors.END}")
    
    def test_system_specs(self):
        """Test system specifications retrieval"""
        print(f"\n{Colors.BOLD}ğŸ’» Testing System Specifications{Colors.END}")
        
        # Test basic system specs
        result = self.run_test(get_system_specs, "Get all system specs")
        self.add_result(result)
        
        # Test individual component specs
        component_tests = [
            ({"get_cpu": True, "get_ram": False, "get_disk": False, "get_network": False, "get_battery": False, "get_gpu": False, "get_os": False}, "CPU specs only"),
            ({"get_cpu": False, "get_ram": True, "get_disk": False, "get_network": False, "get_battery": False, "get_gpu": False, "get_os": False}, "RAM specs only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": True, "get_network": False, "get_battery": False, "get_gpu": False, "get_os": False}, "Disk specs only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": False, "get_network": True, "get_battery": False, "get_gpu": False, "get_os": False}, "Network specs only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": False, "get_network": False, "get_battery": True, "get_gpu": False, "get_os": False}, "Battery specs only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": False, "get_network": False, "get_battery": False, "get_gpu": False, "get_os": True}, "OS specs only"),
        ]
        
        # Add GPU test for Windows
        if self.current_os == "Windows":
            component_tests.append(({"get_cpu": False, "get_ram": False, "get_disk": False, "get_network": False, "get_battery": False, "get_gpu": True, "get_os": False}, "GPU specs only (Windows)"))
        
        for kwargs, description in component_tests:
            result = self.run_test(get_system_specs, description, **kwargs)
            self.add_result(result)
        
        # Test combined components
        combined_tests = [
            ({"get_cpu": True, "get_ram": True, "get_disk": False, "get_network": False, "get_battery": False, "get_gpu": False, "get_os": False}, "CPU + RAM specs"),
            ({"get_cpu": True, "get_ram": True, "get_disk": True, "get_network": True, "get_battery": False, "get_gpu": False, "get_os": False}, "Core components"),
        ]
        
        for kwargs, description in combined_tests:
            result = self.run_test(get_system_specs, description, **kwargs)
            self.add_result(result)
    
    def test_hardware_usage(self):
        """Test real-time hardware usage monitoring"""
        print(f"\n{Colors.BOLD}ğŸ“Š Testing Hardware Usage Monitoring{Colors.END}")
        
        # Test basic hardware usage
        result = self.run_test(get_hardware_usage, "Get all hardware usage")
        self.add_result(result)
        
        # Test individual component usage
        component_tests = [
            ({"get_cpu": True, "get_ram": False, "get_disk": False, "get_network": False, "get_battery": False}, "CPU usage only"),
            ({"get_cpu": False, "get_ram": True, "get_disk": False, "get_network": False, "get_battery": False}, "RAM usage only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": True, "get_network": False, "get_battery": False}, "Disk usage only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": False, "get_network": True, "get_battery": False}, "Network usage only"),
            ({"get_cpu": False, "get_ram": False, "get_disk": False, "get_network": False, "get_battery": True}, "Battery usage only"),
        ]
        
        for kwargs, description in component_tests:
            result = self.run_test(get_hardware_usage, description, **kwargs)
            self.add_result(result)
        
        # Test combined components
        combined_tests = [
            ({"get_cpu": True, "get_ram": True, "get_disk": False, "get_network": False, "get_battery": False}, "CPU + RAM usage"),
            ({"get_cpu": True, "get_ram": True, "get_disk": True, "get_network": True, "get_battery": False}, "Core components usage"),
        ]
        
        for kwargs, description in combined_tests:
            result = self.run_test(get_hardware_usage, description, **kwargs)
            self.add_result(result)
    
    def test_process_monitoring(self):
        """Test process monitoring and analysis"""
        print(f"\n{Colors.BOLD}âš™ï¸ Testing Process Monitoring{Colors.END}")
        
        # Test basic process monitoring
        process_tests = [
            ((5, "cpu"), "Top 5 CPU processes"),
            ((10, "cpu"), "Top 10 CPU processes"),
            ((5, "mem"), "Top 5 memory processes"),
            ((10, "mem"), "Top 10 memory processes"),
            ((3, "cpu"), "Top 3 CPU processes"),
            ((1, "cpu"), "Single top CPU process"),
            ((20, "cpu"), "Top 20 CPU processes"),
        ]
        
        for args, description in process_tests:
            result = self.run_test(get_top_n_processes, description, *args)
            self.add_result(result)
        
        # Test edge cases
        edge_cases = [
            ((0, "cpu"), "Zero processes (edge case)"),
            ((-1, "cpu"), "Negative process count (edge case)"),
            ((1000, "cpu"), "Very large process count"),
        ]
        
        for args, description in edge_cases:
            result = self.run_test(get_top_n_processes, description, *args)
            self.add_result(result)
        
        # Test invalid process types
        invalid_tests = [
            ((5, "invalid"), "Invalid process type"),
            ((5, ""), "Empty process type"),
            ((5, None), "None process type"),
        ]
        
        for args, description in invalid_tests:
            result = self.run_test(get_top_n_processes, description, *args)
            # These should fail gracefully or handle the error
            self.add_result(result)
    
    def test_temperature_monitoring(self):
        """Test temperature sensor readings"""
        print(f"\n{Colors.BOLD}ğŸŒ¡ï¸ Testing Temperature Monitoring{Colors.END}")
        
        # Test basic temperature reading
        result = self.run_test(get_system_temps, "Get system temperatures")
        self.add_result(result)
        
        # Test multiple calls to ensure consistency
        for i in range(3):
            result = self.run_test(get_system_temps, f"Temperature reading #{i+1}")
            self.add_result(result)
    
    def test_health_scoring(self):
        """Test system health scoring"""
        print(f"\n{Colors.BOLD}ğŸ¥ Testing System Health Scoring{Colors.END}")
        
        # Test basic health score
        result = self.run_test(system_health_score, "Basic health score")
        self.add_result(result)
        
        # Test CLI version (detailed scores)
        result = self.run_test(system_health_score, "Detailed health score (CLI version)", cliVersion=True)
        self.add_result(result)
        
        # Test multiple calls to ensure consistency
        for i in range(3):
            result = self.run_test(system_health_score, f"Health score consistency test #{i+1}")
            self.add_result(result)
    
    def test_benchmarking(self):
        """Test performance benchmarking"""
        print(f"\n{Colors.BOLD}ğŸ Testing Performance Benchmarking{Colors.END}")
        
        # Test CPU benchmark
        result = self.run_test(cpu_benchmark, "CPU performance benchmark")
        self.add_result(result)
        
        # Test memory benchmark
        result = self.run_test(mem_benchmark, "Memory performance benchmark")
        self.add_result(result)
        
        # Test disk benchmark
        result = self.run_test(disk_benchmark, "Disk performance benchmark")
        self.add_result(result)
        
        # Test benchmark consistency (run each twice)
        consistency_tests = [
            (cpu_benchmark, "CPU benchmark consistency"),
            (mem_benchmark, "Memory benchmark consistency"),
            (disk_benchmark, "Disk benchmark consistency"),
        ]
        
        for benchmark_func, description in consistency_tests:
            result = self.run_test(benchmark_func, description)
            self.add_result(result)
    
    def test_export_functionality(self):
        """Test export functionality"""
        print(f"\n{Colors.BOLD}ğŸ’¾ Testing Export Functionality{Colors.END}")
        
        # Create test functions to export
        def test_data_dict():
            return {"test_key": "test_value", "number": 42, "list": [1, 2, 3]}
        
        def test_data_list():
            return [{"name": "test1", "value": 1}, {"name": "test2", "value": 2}]
        
        def test_specs_data():
            return get_system_specs()
        
        def test_usage_data():
            return get_hardware_usage()
        
        # Test JSON exports
        json_tests = [
            (test_data_dict, False, "Export dict to JSON"),
            (test_data_list, False, "Export list to JSON"),
            (test_specs_data, False, "Export system specs to JSON"),
            (test_usage_data, False, "Export usage data to JSON"),
        ]
        
        for func, csv_flag, description in json_tests:
            result = self.run_test(export_into_file, description, func, csv=csv_flag)
            self.add_result(result)
        
        # Test CSV exports
        csv_tests = [
            (test_data_dict, True, "Export dict to CSV"),
            (test_data_list, True, "Export list to CSV"),
            (test_specs_data, True, "Export system specs to CSV"),
            (test_usage_data, True, "Export usage data to CSV"),
        ]
        
        for func, csv_flag, description in csv_tests:
            result = self.run_test(export_into_file, description, func, csv=csv_flag)
            self.add_result(result)
        
        # Test with invalid data
        def invalid_data():
            return object()  # Non-serializable object
        
        result = self.run_test(export_into_file, "Export invalid data (should handle gracefully)", invalid_data, csv=False)
        self.add_result(result)
    
    def test_error_handling(self):
        """Test error handling and edge cases"""
        print(f"\n{Colors.BOLD}ğŸš¨ Testing Error Handling{Colors.END}")
        
        # Test with mock failures
        with patch('statz.stats.platform.system', return_value="UnsupportedOS"):
            result = self.run_test(get_system_specs, "Unsupported OS handling")
            self.add_result(result)
        
        # Test with invalid parameters
        invalid_tests = [
            (lambda: get_top_n_processes("invalid", "cpu"), "Invalid process count type"),
            (lambda: get_top_n_processes(5, 123), "Invalid process type"),
            (lambda: system_health_score(cliVersion="invalid"), "Invalid cliVersion parameter"),
        ]
        
        for test_func, description in invalid_tests:
            result = self.run_test(test_func, description)
            self.add_result(result)
    
    def test_stress_performance(self):
        """Test stress and performance scenarios"""
        print(f"\n{Colors.BOLD}ğŸ’ª Testing Stress & Performance{Colors.END}")
        
        # Test rapid consecutive calls
        rapid_tests = [
            (get_hardware_usage, "Rapid hardware usage calls"),
            (get_system_specs, "Rapid system specs calls"),
            (lambda: get_top_n_processes(5, "cpu"), "Rapid process monitoring calls"),
        ]
        
        for test_func, description in rapid_tests:
            start_time = time.time()
            errors = 0
            
            for i in range(10):  # 10 rapid calls
                try:
                    test_func()
                except Exception:
                    errors += 1
            
            duration = time.time() - start_time
            passed = errors == 0
            
            result = TestResult(
                f"{description} (10 calls)",
                passed,
                f"Completed in {duration:.2f}s, {errors} errors",
                f"{errors} errors occurred" if errors > 0 else "",
                duration
            )
            self.add_result(result)
        
        # Test memory usage over time
        start_time = time.time()
        try:
            for i in range(5):
                get_hardware_usage()
                get_system_specs()
                time.sleep(0.1)  # Small delay
            
            duration = time.time() - start_time
            result = TestResult(
                "Memory usage over time test",
                True,
                f"Completed in {duration:.2f}s",
                "",
                duration
            )
        except Exception as e:
            duration = time.time() - start_time
            result = TestResult(
                "Memory usage over time test",
                False,
                "",
                str(e),
                duration
            )
        
        self.add_result(result)
    
    def test_data_validation(self):
        """Test data structure validation"""
        print(f"\n{Colors.BOLD}ğŸ” Testing Data Validation{Colors.END}")
        
        # Test system specs structure
        try:
            specs = get_system_specs()
            
            # Validate specs structure - should be a list
            if not isinstance(specs, list):
                result = TestResult(
                    "System specs structure validation",
                    False,
                    f"Expected list, got {type(specs).__name__}",
                    "System specs should return a list",
                    0.0
                )
            else:
                # Check if we have reasonable data
                has_reasonable_data = len(specs) > 0
                valid_structure = True
                validation_details = []
                
                result = TestResult(
                    "System specs structure validation",
                    has_reasonable_data and valid_structure,
                    f"Specs structure: {', '.join(validation_details)}",
                    "Invalid data types in specs structure" if not valid_structure else 
                    "Empty specs dictionary" if not has_reasonable_data else "",
                    0.0
                )
            self.add_result(result)
        except Exception as e:
            result = TestResult(
                "System specs structure validation",
                False,
                "",
                str(e),
                0.0
            )
            self.add_result(result)
        
        # Test hardware usage structure
        try:
            usage = get_hardware_usage()
            
            # Validate usage structure (should be a list of 5 elements)
            has_expected_structure = isinstance(usage, list) and len(usage) == 5
            
            result = TestResult(
                "Hardware usage structure validation",
                has_expected_structure,
                f"Usage structure: {type(usage).__name__} with {len(usage) if isinstance(usage, list) else 'N/A'} elements",
                "Invalid usage structure" if not has_expected_structure else "",
                0.0
            )
            self.add_result(result)
        except Exception as e:
            result = TestResult(
                "Hardware usage structure validation",
                False,
                "",
                str(e),
                0.0
            )
            self.add_result(result)
        
        # Test benchmark structure
        benchmark_tests = [
            (cpu_benchmark, "CPU benchmark structure", ["execution_time", "score"]),
            (mem_benchmark, "Memory benchmark structure", ["execution_time", "score"]),
            (disk_benchmark, "Disk benchmark structure", ["overall_score"]),
        ]
        
        for benchmark_func, description, required_keys in benchmark_tests:
            try:
                benchmark_result = benchmark_func()
                
                has_required_keys = isinstance(benchmark_result, dict)
                for key in required_keys:
                    has_required_keys = has_required_keys and key in benchmark_result
                
                result = TestResult(
                    description,
                    has_required_keys,
                    f"Keys: {list(benchmark_result.keys()) if isinstance(benchmark_result, dict) else 'Invalid'}",
                    f"Missing required keys: {required_keys}" if not has_required_keys else "",
                    0.0
                )
                self.add_result(result)
            except Exception as e:
                result = TestResult(
                    description,
                    False,
                    "",
                    str(e),
                    0.0
                )
                self.add_result(result)
    
    def cleanup_test_files(self):
        """Clean up any test files created during export tests"""
        patterns = ["statz_export_*.json", "statz_export_*.csv"]
        for pattern in patterns:
            files = glob.glob(pattern)
            for file in files:
                try:
                    os.remove(file)
                except OSError:
                    pass  # Ignore if file doesn't exist or can't be removed
    
    def print_summary(self):
        """Print test summary"""
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r.passed)
        failed_tests = total_tests - passed_tests
        
        print("\n" + "=" * 60)
        print(f"{Colors.BOLD}ğŸ“Š TEST SUMMARY{Colors.END}")
        print("=" * 60)
        
        print(f"Total Tests: {total_tests}")
        print(f"{Colors.GREEN}Passed: {passed_tests}{Colors.END}")
        print(f"{Colors.RED}Failed: {failed_tests}{Colors.END}")
        print(f"Success Rate: {(passed_tests/total_tests)*100:.1f}%")
        
        total_duration = sum(r.duration for r in self.results)
        print(f"Total Duration: {total_duration:.2f}s")
        
        if failed_tests > 0:
            print(f"\n{Colors.RED}âŒ Failed Tests:{Colors.END}")
            for result in self.results:
                if not result.passed:
                    print(f"  â€¢ {result.name}: {result.error}")
        
        print("\n" + "=" * 60)
        
        # Overall result
        if failed_tests == 0:
            print(f"{Colors.GREEN}ğŸ‰ All tests passed! Module is working correctly.{Colors.END}")
            return True
        else:
            print(f"{Colors.RED}âš ï¸ {failed_tests} test(s) failed. Please review the errors above.{Colors.END}")
            return False
    
    def run_all_tests(self):
        """Run the complete test suite"""
        print(f"{Colors.BOLD}ğŸš€ Running Complete Module Test Suite{Colors.END}\n")
        
        try:
            self.test_system_specs()
            self.test_hardware_usage()
            self.test_process_monitoring()
            self.test_temperature_monitoring()
            self.test_health_scoring()
            self.test_benchmarking()
            self.test_export_functionality()
            self.test_error_handling()
            self.test_data_validation()
            self.test_stress_performance()
        finally:
            self.cleanup_test_files()
        
        return self.print_summary()
    
    def run_specs_tests(self):
        """Run only system specs tests"""
        print(f"{Colors.BOLD}ğŸ’» Running System Specs Tests Only{Colors.END}\n")
        self.test_system_specs()
        self.test_data_validation()
        return self.print_summary()
    
    def run_usage_tests(self):
        """Run only hardware usage tests"""
        print(f"{Colors.BOLD}ğŸ“Š Running Hardware Usage Tests Only{Colors.END}\n")
        self.test_hardware_usage()
        return self.print_summary()
    
    def run_process_tests(self):
        """Run only process monitoring tests"""
        print(f"{Colors.BOLD}âš™ï¸ Running Process Tests Only{Colors.END}\n")
        self.test_process_monitoring()
        return self.print_summary()
    
    def run_temp_tests(self):
        """Run only temperature tests"""
        print(f"{Colors.BOLD}ğŸŒ¡ï¸ Running Temperature Tests Only{Colors.END}\n")
        self.test_temperature_monitoring()
        return self.print_summary()
    
    def run_health_tests(self):
        """Run only health scoring tests"""
        print(f"{Colors.BOLD}ğŸ¥ Running Health Tests Only{Colors.END}\n")
        self.test_health_scoring()
        return self.print_summary()
    
    def run_benchmark_tests(self):
        """Run only benchmarking tests"""
        print(f"{Colors.BOLD}ğŸ Running Benchmark Tests Only{Colors.END}\n")
        self.test_benchmarking()
        return self.print_summary()
    
    def run_export_tests(self):
        """Run only export functionality tests"""
        print(f"{Colors.BOLD}ğŸ’¾ Running Export Tests Only{Colors.END}\n")
        try:
            self.test_export_functionality()
        finally:
            self.cleanup_test_files()
        return self.print_summary()
    
    def run_stress_tests(self):
        """Run only stress/performance tests"""
        print(f"{Colors.BOLD}ğŸ’ª Running Stress Tests Only{Colors.END}\n")
        self.test_stress_performance()
        return self.print_summary()

def main():
    """Main test runner"""
    parser = argparse.ArgumentParser(description="statz Module Test Suite")
    parser.add_argument("--specs", action="store_true", help="Run system specs tests only")
    parser.add_argument("--usage", action="store_true", help="Run hardware usage tests only")
    parser.add_argument("--processes", action="store_true", help="Run process monitoring tests only")
    parser.add_argument("--temp", action="store_true", help="Run temperature tests only")
    parser.add_argument("--health", action="store_true", help="Run health scoring tests only")
    parser.add_argument("--benchmark", action="store_true", help="Run benchmarking tests only")
    parser.add_argument("--export", action="store_true", help="Run export functionality tests only")
    parser.add_argument("--stress", action="store_true", help="Run stress/performance tests only")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    tester = ModuleTester(verbose=args.verbose)
    
    success = True
    
    if args.specs:
        success = tester.run_specs_tests()
    elif args.usage:
        success = tester.run_usage_tests()
    elif args.processes:
        success = tester.run_process_tests()
    elif args.temp:
        success = tester.run_temp_tests()
    elif args.health:
        success = tester.run_health_tests()
    elif args.benchmark:
        success = tester.run_benchmark_tests()
    elif args.export:
        success = tester.run_export_tests()
    elif args.stress:
        success = tester.run_stress_tests()
    else:
        success = tester.run_all_tests()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
